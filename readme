• In a groundbreaking 2017 paper (https://arxiv.org/abs/1706.03762), entitled: "Attention is All You Need" a team of Google researchers suggested a new much more efficient architecture for NMT models.
• They introduced an architecture called the Transformer, which significantly improved the state of the art in NMT without using any recurrent or convolutional layers, just attention mechanisms (plus embedding layers, dense layers, normalization layers, and a few other bits and pieces).
• As an extra bonus, this architecture was also much faster to train and easier to parallelize, so they managed to train it at a fraction of the time and cost of the previous state-of-the-art models.
